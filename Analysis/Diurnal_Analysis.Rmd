---
title: "Analysis of Casco Bay OA data through 2018 -- Diurnal Patterns "
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />


# Introduction
This notebook and related notebooks document analysis of data derived from a 
multi-year deployment of ocean acidification monitoring equipment at the 
Southern Maine Community College pier, in South Portland.

The monitoring set up was designed and operated by Joe Kelly, of UNH and his
colleagues, on behalf of the Casco Bay Estuary Partnership.  This was one of the
first long-term OA monitoring facilities in the northeast, and was intended to
test available technologies as well as gain operational experience working with
acidification monitoring.

In this Notebook, we develop formal Generalized Additive Model analyses of 
diurnal patterns, addressing autocorrelation.

# Load Libraries
```{r load_libraries}
library(tidyverse)  # includes readr, readxl
library(lubridate)
library(mgcv)

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())

```

# Color Palette
For seasonal displays
This is just a lists, not a function.
```{r}
season_palette = c(cbep_colors()[1],
                    cbep_colors()[4],
                    cbep_colors()[2],
                    'orange')
```

# Load Data
## Establish Folder References
```{r folder_refs}
sibfldnm <- 'Derived_Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

fn    <- 'CascoBayOAData.csv'
fpath <- file.path(sibling,fn)
```

The following loads existing data, including a "temperature corrected" pCO2 
value based on Takahashi et al. 2002. It then collapses that data to daily 
summaries.

> Takahashi, Taro & Sutherland, Stewart & Sweeney, Colm & Poisson, Alain &
> Metzl, Nicolas & Tilbrook, Bronte & Bates, Nicholas & Wanninkhof, Rik & Feely,
> Richard & Chris, Sabine & Olafsson, Jon & Nojiri, Yukihiro. (2002). Global
> sea-air CO2 flux based on climatological surface ocean pCO2, and seasonal
> biological and temperature effects. Deep Sea Research Part II: Topical Studies
> in Oceanography. 49. 1601-1622. 10.1016/S0967-0645(02)00003-6.

(See the "Data_Review_And_Filtering" R Notebook for details on why and how we 
calculated temperature-corrected pCO2 values.)

## Read Data
We add the Month and Season factor here for use in later graphics.

Note that the original time coordinate here is in UTC, not local time. But by
default, read_csv() interprets times according to the locale, here Eastern
Standard Time or Eastern Daylight Time, depending on time of year.  I have not
found an easy way to alter that behavior, but the force_tz() function in
lubridate can fix it.

Local time is more appropriate for graphics showing the effect of time of day on 
acidification parameters.  

```{r load_data} 
all_data <- read_csv(fpath,
                     col_types = cols(dd = col_integer(), 
                                      doy = col_integer(),
                                      hh = col_integer(),
                                      mm = col_integer(),
                                      yyyy = col_integer())) %>%
  select(c(datetime, ph, co2, co2_corr)) %>%
  mutate(datetime = force_tz(datetime, tzone = 'UTC')) %>%
  
  # Calculate local standard time coordinates
  mutate(stdtime = structure(datetime, tzone = 'Etc/GMT+5')) %>%
  mutate(yyyy  = as.numeric(format(stdtime, format = '%Y')),
         mm    = as.numeric(format(stdtime, format = '%m')),
         dd    = as.numeric(format(stdtime, format = '%d')),
         doy   = as.numeric(format(stdtime, format = '%j')),
         hh    = as.numeric(format(stdtime, format = '%H')),
         Month = factor(mm, levels=1:12, labels = month.abb)
         ) %>%
  mutate(Season = recode_factor(mm, 
                                `1`  = 'Winter',
                                `2`  = 'Winter',
                                `3`  = 'Spring',
                                `4`  = 'Spring',
                                `5`  = 'Spring',
                                `6`  = 'Summer',
                                `7`  = 'Summer',
                                `8`  = 'Summer',
                                `9`  = 'Fall',
                                `10` = 'Fall',
                                `11` = 'Fall',
                                `12` = 'Winter'
                                ))
```

## Diurnal Deviations
### Calculate Deviations  
```{r hourly_deviations}
diurnal_data <- all_data %>%
  group_by(yyyy, mm, dd) %>%
  # Calculate sample sizes for each day.
  mutate(co2_n      = sum(! is.na(co2)),
         co2_corr_n = sum(! is.na(co2_corr)),
         ph_n       = sum(! is.na(ph))) %>%
  
  # Calculate centered but not scaled values, day by day.
  mutate(co2_res      = scale(co2, scale = FALSE),
         co2_corr_res = scale(co2_corr, scale = FALSE),
         ph_res       = scale(ph, scale = FALSE)) %>%
  ungroup(yyyy, mm, dd) %>%
  
  # Replace data from any days with less than 20 hours of data with NA
  mutate(co2_res      = ifelse(co2_n>=20, co2_res, NA),
         co2_corr_res = ifelse(co2_corr_n>=20, co2_corr_res, NA),
         ph_res       = ifelse(ph_n>=20, ph_res, NA)) %>%

  # Delete unnecessary data to save space
  select(-contains('_n')) %>%
  select(-c(datetime, mm, ph, co2, co2_corr))
```

# Analyze pCO~2~ (Temperature Corrected) by Season
## Initial GAM model
We fit a simple model, that fits a GAM smoother (cyclic cubic spline) to time of 
day while accounting for autocorrelation. This model does NOT fit autocorrelated
errors.

Note also that we do not include linear predictor main effects for the seasons
here.  That is because we know, *a priori*, that the average deviation from 
daily averages is zero, so there is intercept term.

```{r pco2_gam}
system.time(pco2_gam <- gam(co2_corr_res ~  s(hh, by = Season, bs='cc'),
                 data = diurnal_data, , na.action = na.omit))
```

```{r pco2_gam_summary}
summary(pco2_gam)
```
### Check The Model
#### Can We Omit Main Effects?
Since our data are centered, the intercepts should be zero. Do our results 
change if we omit the seasonal main effects from the model?
```{r pco2_gam_b}
system.time(pco2_gam_b <- gam(co2_corr_res ~  Season + s(hh, by = Season, bs='cc'),
                 data = diurnal_data, na.action = na.omit))
```

```{r pco2_gamm_summary_gam_b}
summary(pco2_gam_b)
rm(pco2_gam_b)
```

#### Check For Violations of Assumptions
```{r check_pco2_gam}
gam.check(pco2_gam)
```

### Autocorrelation
```{r pco2_autocorrelation}
pacf(resid(pco2_gam))
```
So, the residuals continue to be large, with an AR(1) term on the order of 0.7.  
Fluctuations in autocorrelations over longer lags appear to reflect the model, 
since residuals show a 24 hour periodicity.

### Predictions from GAM
```{r predictions}
newdat <- expand.grid(hh = seq(0, 23),
                    Season = c('Winter', 'Spring', 'Summer', 'Fall'))

p <- predict(pco2_gam, newdata = newdat, se.fit=TRUE)
newdat <- newdat %>%
  mutate(pred = p$fit, se = p$se.fit)
```

### Ribbon Plot from GAM
These ribbon plots show approximate 95% confidence intervals for the spline fits 
by season.  Here those confidence limits are deeply suspect, because the model 
does not take into account autocorrelation.
```{r ribbon_plot_pco2_gam, fig.width = 4, fig.height = 4}
ggplot(newdat, aes(x=hh, y=pred, color = Season)) + #geom_line() +
  geom_ribbon(aes(ymin = pred-(1.96*se),
                  ymax = pred+(1.96*se),
                  fill = Season), alpha = 0.5,
              color = NA) +
  
  theme_cbep(base_size= 12) +
  theme(legend.key.width = unit(0.25,"in"),
        legend.text      = element_text(size = 8)) +
  scale_fill_manual(values = season_palette, name = '') +
  
  xlab('Hour of Day') +
  ylab(expression (atop(pCO[2*(cor)]~(mu*Atm), Difference~From~Daily~Average))) 

```

## Initial GAMM
We want to fit an AR1() error term.  We tried using an AR(1) using the whole
data set in one block, but that crashed my computer, requiring too much memory.

We also tried running a GAMM with an autocorrelation function set up to depend
on an hourly time stamp, but it, too took too much memory, and crashed.

The simple autocorrelation function , which looks for correlations with
successive observations (ignoring any recorded time coordinates), would be
identical to  one based on a time stamp, IF there were no missing values.

Interestingly, it appears that the corAR1 function, when the parameter 
form = NULL,  does not consider missing values at all.

To reduce memory requirements, we can look for autocorrelation structure in 
data subsets.  Here we use look for sequential (not temporal) autocorrelations
within seasons.  Since there are very few season transitions, this should fit 
almost the same model as fitting a single autocorrelation structure across the 
whole data set.

```{r cleanup}
rm(all_data)
rm(m, p, pco2_gam)
```

The following take ~ 15 minutes on a lightly loaded machine, and as long as 25
min on a machine with lots of other things going on.
```{r cache = TRUE} 
system.time(pco2_gam2 <- gamm(co2_corr_res ~  s(hh, by = Season, bs='cc', k=6),
                 correlation = corAR1(form = ~ 1 | Season),  # we run out of memory if we don't use a grouping
                 data = diurnal_data))
```


```{r summary_pco2gamm_gam}
summary(pco2_gam2$gam)
```
Note the very low overall R squared here.  This simple model explains less than
one fifth of the variation in deviations from daily average pCO~2~.

```{r summary_pco2gamm_lme}
summary(pco2_gam2$lme)
```
The primary insight from the LME summary is that the AR1 autocorrelation is 
estimated to be phi =  0.726214.

###  Model Predictions
```{r pco2_gam_predictions}
p <- predict(pco2_gam2$gam, newdata = newdat, se.fit=TRUE)
m <- predict(pco2_gam2$gam, newdata = newdat, type ='lpmatrix')
newdat <- newdat %>%
  mutate(pred2 = p$fit, se2 = p$se.fit)
```

#### Predictions are nearly identical
```{r}
newdat %>%
ggplot(aes(x=pred, y=pred2)) + 
  geom_point(aes(color=Season)) + 
  geom_abline(aes(slope = 1,intercept = 0))
```

#### But standard Errors are different
```{r}
newdat %>%
ggplot(aes(x=se, y=se2)) + geom_point(aes(color=Season)) +
  geom_abline(aes(slope = 1,intercept = 0)) +
  coord_cartesian(xlim = c(0,1.75), ylim = c(0,2.5)) +
  xlab('Standard Error\nfrom Model without Autocorrelation') +
  ylab('Standard Error\nfrom Model with Autocorrelation')
```

###  Ribbon Graphic
These ribbon plots show approximate 95% confidence intervals for the spline fits
by season.
```{r ribbon_graphic, fig.width = 4, fig.height = 4}
ggplot(newdat, aes(x=hh, y=pred2, color = Season)) + #geom_line() +
  geom_ribbon(aes(ymin = pred2-(1.96*se2),
                  ymax = pred2+(1.96*se2),
                  fill = Season), alpha = 0.5,
              color = NA) +
  
  theme_cbep(base_size= 12) +
  theme(legend.key.width = unit(0.25,"in"),
        legend.text      = element_text(size = 8)) +
  scale_fill_manual(values = season_palette, name = '') +
  
  xlab('Hour of Day') +
  ylab(expression (atop(pCO[2*(cor)]~(mu*Atm), Difference~From~Daily~Average))) 

```

## Formal comparisons of differences between GAM spline fits
We follow ideas from (here)[https://fromthebottomoftheheap.net/2017/10/10/difference-splines-i/],
which describes formal piecewise comparison of spline fits.  The idea is to 
manually calculate standard errors for a difference between two estimators, 
here the difference between seasonal values by hour.

predict.gam returns an "lpmatrix" which
>  "yields the values of the linear predictor (minus any offset) when 
   postmultiplied by the parameter vector....  The latter option is most useful 
   for getting variance estimates for quantities derived from the model...."

That is how we use it here.

Note that the matrix is determined, in part, by the  type of contrasts used to 
fit the model.  Since we did not specify, the default treatment contrasts were 
used, comparing each level (Season) to the first level (Winter).

An alternative approach to hypothesis testing, which uses different model 
contrasts to test more holistically for differences between prediction curves 
is presented (here)[https://jacolienvanrij.com/Tutorials/GAMM.html].

```{r setup_formal_comparison}
## Which columns of m correspond to seasons of interest?
c1 <- grepl('Winter', colnames(m))
c2 <- grepl('Spring', colnames(m))
c3 <- grepl('Summer', colnames(m))
c4 <- grepl('Fall', colnames(m))

## Which rows of m correspond to seasons of interest?
r1 <- with(newdat, Season == 'Winter')
r2 <- with(newdat, Season == 'Spring')
r3 <- with(newdat, Season == 'Summer')
r4 <- with(newdat, Season == 'Fall')
```


```{r example_comparison}

#Example -- using matrix algebra to calculate estimates and standard errors
# Create a row vector of spline fits
mm_wsp <- m[r1,] - m[r2,]             # differences between winter and spring subparts of the matrix
mm_wsp[, ! (c1 | c2)] <- 0            # Set other columns to zero
dif <- mm_wsp %*% coef(pco2_gam2$gam) # Use Matrix multiplication to estimate

# We work with the normal distribution here, because with our huge time series
# sample sizes, the difference between the normal distribution and the
# t distribution is negligible.
se <- sqrt(rowSums((mm_wsp %*% vcov(pco2_gam2$gam, unconditional=TRUE)) * mm_wsp))
upr <- dif + (qnorm(0.975) * se)
lwr <- dif - (qnorm(0.975) * se)
```


```{r plot_comparison}
ggplot(data = NULL, aes(x = 0:23)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = cbep_colors()[1], alpha = 0.5) +
  geom_hline(yintercept = 0) +
  theme_cbep()
```
So, apparent differences between winter and spring diurnal patterns are unlikely
to be due to chance alone.  Although one should be careful with this analysis.
This approach implicitly makes multiple hourly comparisons, thus potentially
inflating apparent statistical significance.

Most other potential contrasts show even larger differences in diurnal patterns.
We will not produce graphics for those.  The other two seasons that provide
similar values for much of the day are summer and fall, yet even here, the
patterns are clearly distinct.

```{r second_comparison}
mm_suf <- m[r3,] - m[r4,]
mm_suf[, ! (c3 | c4)] <- 0
dif <- mm_wsp %*% coef(pco2_gam2$gam)  # Use Matrix multiplication to estimate

se <- sqrt(rowSums((mm_suf %*% 
                      vcov(pco2_gam2$gam, unconditional=TRUE)) * mm_suf))
upr <- dif + (qnorm(0.975) * se)
lwr <- dif - (qnorm(0.975) * se)
ggplot(data = NULL, aes(x = 0:23)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = cbep_colors()[1], alpha = 0.5) +
  geom_hline(yintercept = 0) +
  theme_cbep()
```


# Other Models
Several other GAMMs models used up all available memory, and could not be run.
Chances are, the limited model we just ran would UNDER estimate autocorrelation,
but it's not clear by how much.  The effect would be to make our estimates of
standard errors too small.

```{r cache = TRUE, eval = FALSE} 
#rm(pco2_gam2)
if (FALSE) {
  system.time(pco2_gam3 <- gamm(co2_corr_res ~  s(hh, by = Season, bs='cc', k=6),
                 correlation = corAR1(form = ~ stdtime | Month),  # we run out of memory if we don't use a grouping
                 data = diurnal_data))
}
```


## Analysis of pH
### Initial GAM model
We fit a simple model, that fits a GAM smoother (cyclic cubic spline) to time of 
day while accounting for autocorrelation. This model does NOT fit autocorrelated 
errors.

Note also that we do not include linear predictor main effects for the seasons 
here.  That is because we know, *a priori*, that the average deviation from 
daily averages is zero, so there is an intercept term.

```{r ph_gam}
system.time(ph_gam <- gam(ph_res ~  s(hh, by = Season, bs='cc'),
                 data = diurnal_data, , na.action = na.omit))
```

```{r ph_gam_summary}
summary(ph_gam)
```
### Check The Model
#### Can We Omit Main Effects?
Since our data are centered, the intercepts should be zero. Do our results 
change if we omit the seasonal main effects from the model?
```{r ph_gam_b}
system.time(ph_gam_b <- gam(ph_res ~  Season + s(hh, by = Season, bs='cc'),
                 data = diurnal_data, na.action = na.omit))
```

```{r ph_gamm_summary_gam_b}
summary(ph_gam_b)
rm(ph_gam_b)
```

#### Check For Violations of Assumptions
```{r check_ph_gam}
gam.check(ph_gam)
```
There appears to be some structure to the residuals, and the residuals are very 
heavy tailed.  I would not trust standard error or P values much coming out of 
this analysis.
### Autocorrelation
```{r ph_autocorrelation}
pacf(resid(ph_gam))
```
So, the residuals continue to be large, with an AR(1) term on the order of 0.7.  
Fluctuations in autocorrelations over longer lags appear to reflect the model, 
since residuals show a 12/24 hour periodicity.

### Predictions from GAM
```{r ph_predictions}
newdat <- expand.grid(hh = seq(0, 23),
                    Season = c('Winter', 'Spring', 'Summer', 'Fall'))

p <- predict(ph_gam, newdata = newdat, se.fit=TRUE)
newdat <- newdat %>%
  mutate(pred = p$fit, se = p$se.fit)
```

### Ribbon Plot from GAM
These ribbon plots show approximate 95% confidence intervals for the spline fits 
by season.  Here those confidence limits are deeply suspect, because the model 
does not take into account autocorrelation.
```{r ribbon_plot_ph_gam, fig.width = 4, fig.height = 4}
ggplot(newdat, aes(x=hh, y=pred, color = Season)) + #geom_line() +
  geom_ribbon(aes(ymin = pred-(1.96*se),
                  ymax = pred+(1.96*se),
                  fill = Season), alpha = 0.5,
              color = NA) +
  
  theme_cbep(base_size= 12) +
  theme(legend.key.width = unit(0.25,"in"),
        legend.text      = element_text(size = 8)) +
  scale_fill_manual(values = season_palette, name = '') +
  
  xlab('Hour of Day') +
  ylab(expression (atop(pH, Difference~From~Daily~Average))) 

```

## Initial GAMM
We want to fit an AR1() error term.  We tried using an AR(1) using the whole 
data set in one block, but that crashed my computer, requiring too much memory.

We also tried running a GAMM with an autocorrelation function set up to depend 
on an hourly time stamp, but it, too took too much memory, and crashed.

The simple autocorrelation function , which looks for correlations with
successive observations (ignoring any recorded time coordinates), would be 
identical to  one based on a time stamp, IF there were no missing values.

Interestingly, it appears that the corAR1 function, when the parameter 
form = NULL,  does not consider missing values at all.

To reduce memory requirements, we can look for autocorrelation structure in 
data subsets.  Here we use look for sequential (not temporal) autocorrelations
within seasons.  Since there are very few season transitions, this should fit 
almost the same model as fitting a single autocorrelation structure across the 
whole data set.

```{r cleanup_2}
rm(all_data)
rm(p, ph_gam)
```

The following take ~ 7 minutes on a lightly loaded machine.
```{r ph_gamm, cache = TRUE} 
system.time(ph_gam <- gamm(ph_res ~  s(hh, by = Season, bs='cc', k=6),
                 correlation = corAR1(form = ~ 1 | Season),  # we run out of memory if we don't use a grouping
                 data = diurnal_data))
```

Revising that to run off the actual time stamp did not work.  It ran for well 
over an hour before crashing.

```{r summary_ph_gamm_gam}
summary(ph_gam$gam)
```
Note the very low overall R squared here.  This simple model explains less than 
one fifth of the variation in deviations from daily average pCO~2~.

```{r summary_ph_gamm_lme}
summary(ph_gam$lme)
```
The primary insight from the LME summary is that the AR1 autocorrelation is 
estimated to be phi =  0.726214.

###  Model Predictions
```{r ph_gamm_predictions}
p <- predict(ph_gam$gam, newdata = newdat, se.fit=TRUE)
m <- predict(ph_gam$gam, newdata = newdat, type ='lpmatrix')
newdat <- newdat %>%
  mutate(pred2 = p$fit, se2 = p$se.fit)
```

#### Predictions are nearly identical
```{r compare_predictions_ph}
newdat %>%
ggplot(aes(x=pred, y=pred2)) + 
  geom_point(aes(color=Season)) + 
  geom_abline(aes(slope = 1,intercept = 0))
```

#### But standard Errors are different
```{r compare_stderr_ph}
newdat %>%
ggplot(aes(x=se, y=se2)) + geom_point(aes(color=Season)) +
  geom_abline(aes(slope = 1,intercept = 0)) +
  coord_cartesian(xlim = c(0,1.75), ylim = c(0,2.5)) +
  xlab('Standard Error\nfrom Model without Autocorrelation') +
  ylab('Standard Error\nfrom Model with Autocorrelation')
```

###  Ribbon Graphic
These ribbon plots show approximate 95% confidence intervals for the spline fits
by season.
```{r ribbon_graphic_ph, fig.width = 4, fig.height = 4}
ggplot(newdat, aes(x=hh, y=pred2, color = Season)) + #geom_line() +
  geom_ribbon(aes(ymin = pred2-(1.96*se2),
                  ymax = pred2+(1.96*se2),
                  fill = Season), alpha = 0.5,
              color = NA) +
  
  theme_cbep(base_size= 12) +
  theme(legend.key.width = unit(0.25,"in"),
        legend.text      = element_text(size = 8)) +
  scale_fill_manual(values = season_palette, name = '') +
  
  xlab('Hour of Day') +
  ylab(expression (atop(pCO[2*(cor)]~(mu*Atm), Difference~From~Daily~Average))) 

```

## Formal comparisons of differences between GAM spline fits
We follow ideas from (here)[https://fromthebottomoftheheap.net/2017/10/10/difference-splines-i/],
which describes formal piecewise comparison of spline fits.  The idea is to 
manually calculate standard errors for a difference between two estimators, 
here the difference between seasonal values by hour.

predict.gam returns an "lpmatrix" which
>  "yields the values of the linear predictor (minus any offset) when 
   postmultiplied by the parameter vector.... The latter option is most useful 
   for getting variance estimates for quantities derived from the model...."

That is how we use it here.

Note that the matrix is determined, in part, by the  type of contrasts used to
fit the model.  Since we did not specify, the default treatment contrasts were
used, comparing each level (Season) to the first level (Winter).

An alternative approach to hypothesis testing, which uses different model 
contrasts to test more holistically for differences between prediction curves 
is presented (here)[https://jacolienvanrij.com/Tutorials/GAMM.html].

```{r setup_formal_comparison_ph}
## Which columns of m correspond to seasons of interest?
c1 <- grepl('Winter', colnames(m))
c2 <- grepl('Spring', colnames(m))
c3 <- grepl('Summer', colnames(m))
c4 <- grepl('Fall', colnames(m))

## Which rows of m correspond to seasons of interest?
r1 <- with(newdat, Season == 'Winter')
r2 <- with(newdat, Season == 'Spring')
r3 <- with(newdat, Season == 'Summer')
r4 <- with(newdat, Season == 'Fall')
```


```{r example_comparison_ph}

#Example -- using matrix algebra to calculate estimates and standard errors
# Create a row vector of spline fits
mm_wsp <- m[r1,] - m[r2,]             # differences between winter and spring subparts of the matrix
mm_wsp[, ! (c1 | c2)] <- 0            # Set other columns to zero
dif <- mm_wsp %*% coef(pco2_gam2$gam) # Use Matrix multiplication to estimate

# We work with the normal distribution here, because with our huge time series
# sample sizes, the difference between the normal distribution and the
# t distribution is negligible.
se <- sqrt(rowSums((mm_wsp %*% vcov(pco2_gam2$gam, unconditional=TRUE)) * mm_wsp))
upr <- dif + (qnorm(0.975) * se)
lwr <- dif - (qnorm(0.975) * se)
```


```{r plot_comparison_ph}
ggplot(data = NULL, aes(x = 0:23)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = cbep_colors()[1], alpha = 0.5) +
  geom_hline(yintercept = 0) +
  theme_cbep()
```
So, apparent differences between winter and spring diurnal patterns are unlikely
to be due to chance alone.  Although one should be careful with this analysis.
This approach implicitly makes multiple hourly comparisons, thus potentially
inflating apparent statistical significance.

Most other potential contrasts show even larger differences in diurnal patterns.
We will not produce graphics for those,.  The other two seasons that provide
similar values for much of the day are summer and fall, yet even here, the
patterns are clearly distinct.

```{r second_comparison_ph}
mm_suf <- m[r3,] - m[r4,]
mm_suf[, ! (c3 | c4)] <- 0
dif <- mm_wsp %*% coef(pco2_gam2$gam)  # Use Matrix multiplication to estimate

se <- sqrt(rowSums((mm_suf %*% vcov(pco2_gam2$gam, 
                                    unconditional=TRUE)) * mm_suf))
upr <- dif + (qnorm(0.975) * se)
lwr <- dif - (qnorm(0.975) * se)
ggplot(data = NULL, aes(x = 0:23)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = cbep_colors()[1], alpha = 0.5) +
  geom_hline(yintercept = 0) +
  theme_cbep()
```
